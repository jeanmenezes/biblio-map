c

, , 1{30 ()
Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.

On the Optimality of the Simple Bayesian
Classi er under Zero-One Loss
PEDRO DOMINGOS
pedrod@ics.uci.edu
MICHAEL PAZZANI
pazzani@ics.uci.edu
Department of Information and Computer Science, University of California, Irvine, CA 92697

Editor: Gregory Provan
Abstract. The simple Bayesian classi er is known to be optimal when attributes are independent
given the class, but the question of whether other su cient conditions for its optimality exist has
so far not been explored. Empirical results showing that it performs surprisingly well in many
domains containing clear attribute dependences suggest that the answer to this question may be
positive. This article shows that, although the Bayesian classi er's probability estimates are only
optimal under quadratic loss if the independence assumption holds, the classi er itself can be
optimal under zero-one loss (misclassi cation rate) even when this assumption is violated by a
wide margin. The region of quadratic-loss optimality of the Bayesian classi er is in fact a secondorder in nitesimal fraction of the region of zero-one optimality. This implies that the Bayesian
classi er has a much greater range of applicability than previously thought. For example, in this
article it is shown to be optimal for learning conjunctions and disjunctions, even though they
violate the independence assumption. Further, studies in arti cial domains show that it will often
outperform more powerful classi ers for common training set sizes and numbers of attributes, even
if its bias is a priori much less appropriate to the domain. This article's results also imply that
detecting attribute dependence is not necessarily the best way to extend the Bayesian classi er,
and this is also veri ed empirically.

Keywords: Simple Bayesian classi er, naive Bayesian classi er, zero-one loss, optimal classi cation, induction with attribute dependences

1. Introduction
In classi cation learning problems, the learner is given a set of training examples
and the corresponding class labels, and outputs a classi er. The classi er takes
an unlabeled example and assigns it to a class. Many classi ers can be viewed
as computing a set of discriminant functions of the example, one for each class,
and assigning the example to the class whose function is maximum (Duda & Hart,
1973). If E is the example, and fi (E) is the discriminant function corresponding
to the ith class, the chosen class Ck is the one for which1
fk (E) > fi (E) 8 i 6= k:

(1)

Suppose an example is a vector of a attributes, as is typically the case in classication applications. Let vjk be the value of attribute Aj in the example, P(X)
denote the probability of X, and P(Y jX) denote the conditional probability of Y
given X. Then one possible set of discriminant functions is

P. DOMINGOS AND M. PAZZANI

2

fi (E) = P(Ci)

a
Y
j =1

P(Aj =vjk jCi):

(2)

The classi er obtained by using this set of discriminant functions, and estimating
the relevant probabilities from the training set, is often called the naive Bayesian
classi er. This is because, if the \naive" assumption is made that the attributes
are independent given the class, this classi er can easily be shown to be optimal,
in the sense of minimizing the misclassi cation rate or zero-one loss, by a direct
application of Bayes' theorem, as follows. If P(CijE) is the probability that example
E is of class Ci , zero-one loss is minimized if, and only if, E is assigned to the class
Ck for which P(Ck jE) is maximum (Duda & Hart, 1973). In other words, using
P(CijE) as the discriminant functions fi (E) is the optimal classi cation procedure.
By Bayes' theorem,
) P(E
(3)
P(CijE) = P(CiP(E) jCi) :
P(E) can be ignored, since it is the same for all classes, and does not a ect the
relative values of their probabilities. If the attributes are independent given the
class, P(E jCi) can be decomposed into the product P(A1 = v1k jCi) . . .P(Aa =
vak jCi), leading to P(CijE) = fi (E), as de ned in Equation 2, Q.E.D.
In practice, attributes are seldom independent given the class, which is why this
assumption is \naive." However, the question arises of whether the Bayesian classi er, as de ned by Equations 1 and 2, can be optimal even when the assumption
of attribute independence does not hold, and therefore P(Ci jE) = fi (E). In these
6
situations, the Bayesian classi er can no longer be said to compute class probabilities given the example, but the discriminant functions de ned by Equation 2
may still minimize misclassi cation error. The question of whether these situations
exist has practical relevance, since the Bayesian classi er has many desirable properties (simplicity, low time and memory requirements, etc.), and thus may well be
the classi er of choice for such situations (i.e., it will be chosen over other classi ers that are also optimal, but di er in other respects). However, even though
the Bayesian classi er has been known for several decades, to our knowledge this
question has so far not been explored; the tacit assumption has always been that
the Bayesian classi er will not be optimal when attribute independence does not
hold.
In spite of this restrictive view of its applicability, in recent years there has been
a gradual recognition among machine learning researchers that the Bayesian classi er can perform quite well in a wide variety of domains, including many where
clear attribute dependences exist. Evidence of the Bayesian classi er's surprising
practical value has also led to attempts to extend it by increasing its tolerance of
attribute independence in various ways, but the success of these attempts has been
uneven. This is described in more detail in the next section.
This article derives the most general conditions for the Bayesian classi er's optimality, giving a positive answer to the question of whether it can still be optimal

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

3

when attributes are not independent given the class. A corollary of these results
is that the Bayesian classi er's true region of optimal performance is in fact far
greater than that implied by the attribute independence assumption, and that its
range of applicability is thus much broader than previously thought. This tolerance
of attribute dependence also helps to explain why extending the Bayesian classier by attempting to reduce it will not necessarily lead to signi cant performance
improvements.
The remainder of the article elaborates on these ideas. Section 2 reviews previous
empirical results on the Bayesian classi er in the machine learning literature, and
recent attempts to extend it. Section 3 describes an empirical study showing that
the Bayesian classi er outperforms several more sophisticated approaches on a large
number of data sets, and that this is not due to the absence of attribute dependences
in those data sets. Section 4 presents a simple example that illustrates some of the
key points to be made subsequently. Section 5 derives necessary and su cient
conditions for the local optimality of the Bayesian classi er (i.e., its optimality for
any given example), and computes how often these conditions will be satis ed.
Section 6 generalizes the previous results to a necessary and su cient condition
for the Bayesian classi er's global optimality (i.e., its optimality for any given data
set). It also shows that the Bayesian classi er has some fundamental limitations,
but is optimal for learning conjunctions and disjunctions. Section 7 formulates some
hypotheses as to when the Bayesian classi er is likely to outperform more exible
ones, even if it is not optimal, and reports empirical tests of these hypotheses.
Section 8 veri es empirically that attempting to reduce attribute dependence is not
necessarily the best approach to improving the Bayesian classi er's accuracy. The
paper concludes with discussion and directions for future work.

2. The simple Bayesian classi er in machine learning
Due to its perceived limitations, the simple Bayesian classi er has traditionally
not been a focus of research in machine learning.2 However, it has sometimes been
used as a \straw man" against which to compare more sophisticated algorithms.
Clark and Niblett (1989) compared it with two rule learners and a decision-tree
learner, and found that it did surprisingly well. Cestnik (1990) reached similar
conclusions. Kononenko (1990) reported that, in addition, at least one class of
users (doctors) nds the Bayesian classi er's representation quite intuitive and easy
to understand, something which is often a signi cant concern in machine learning.
Langley, Iba, and Thompson (1992) compared the Bayesian classi er with a decision
tree learner, and found it was more accurate in four of the ve data sets used.
Pazzani, Muramatsu, and Billsus (1996) compared several learners on a suite of
information ltering tasks, and found that the Bayesian classi er was the most
accurate one overall.
John and Langley (1995) showed that the Bayesian classi er's performance can
be much improved if the traditional treatment of numeric attributes, which assumes
Gaussian distributions, is replaced by kernel density estimation. This showed that

4

P. DOMINGOS AND M. PAZZANI

the Bayesian classi er's limited performance in many domains was not in fact intrinsic to it, but due to the additional use of unwarranted Gaussian assumptions.
Dougherty, Kohavi, and Sahami (1995) reached similar conclusions by instead discretizing numeric attributes, and found the Bayesian classi er with discretization
slightly outperformed a decision-tree learner in 16 data sets, on average.
Although the reasons for the Bayesian classi er's good performance were not
clearly understood, these results were evidence that it might constitute a good
starting point for further development. Accordingly, several authors attempted to
extend it by addressing its main perceived limitation|its inability to deal with
attribute dependences.
Langley and Sage (1994) argued that, when two attributes are correlated, it
might be better to delete one attribute than to assume the two are conditionally
independent. They found that an algorithm for feature subset selection (forward
sequential selection) improved accuracy on some data sets, but had little or no
e ect in others. In a related approach, Kubat, Flotzinger, and Pfurtscheller (1993)
found that using a decision-tree learner to select features for use in the Bayesian
classi er gave good results in the domain of EEG signal classi cation.
Kononenko (1991) proposed successively joining dependent attribute values, using
a statistical test to judge whether two attribute values are signi cantly dependent.
Experimental results with this method were not encouraging. On two domains, the
modi ed Bayesian classi er had the same accuracy as the simple Bayesian classi er,
and on the other two domains tested, the modi ed version was one percent more
accurate, but it is not clear whether this di erence was statistically signi cant.
Pazzani (1996) proposed joining attributes instead of attribute values. Rather
than using a statistical test, as in Kononenko (1991), Pazzani's algorithm used
cross-validation to estimate the accuracy of a classi er with each possible join, and
made the single change that most improved accuracy. This process was repeated
until no change resulted in an improvement. This approach substantially improved
the accuracy of the Bayesian classi er on several arti cial and natural data sets,
with the largest improvements in accuracy occurring in data sets where the Bayesian
classi er is substantially less accurate than decision-tree learners.
The simple Bayesian classi er is limited in expressiveness in that it can only
create linear frontiers (Duda & Hart, 1973). Therefore, even with many training
examples and no noise, it does not approach 100% accuracy on some problems.
Langley (1993) proposed the use of \recursive Bayesian classi ers" to address this
limitation. In his approach, the instance space is recursively divided into subregions
by a hierarchical clustering process, and a Bayesian classi er is induced for each
region. Although the algorithm worked on an arti cial problem, it did not provide
a signi cant bene t on any natural data sets. In a similar vein, Kohavi (1996)
formed decision trees with Bayesian classi ers at the nodes, and showed that it
tended to outperform either approach alone, especially on large data sets.
Friedman, Geiger, and Goldszmidt (1997) compared the simple Bayesian classi er
with Bayesian networks, a much more powerful representation that has the Bayesian
classi er as a special case, and found that the latter approach tended to produce no

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

5

improvements, and sometimes led to large reductions in accuracy. This led them
to attempt a much more limited extension, allowing each attribute to depend on
at most one other attribute (in addition to the class). This conservative approach
achieved the best overall results. Sahami (1996) proposed a related scheme, and,
in a similar spirit, Singh and Provan (1995, 1996) obtained good results by forming
Bayesian networks using only a subset of the attributes.
In summary, the Bayesian classi er has repeatedly performed better than expected in empirical trials, but attempts to build on this success by relaxing the
independence assumption have had mixed results. Both these observations seem to
con ict with the current theoretical understanding of the Bayesian classi er. This
article seeks to resolve this apparent contradiction.

3. Empirical evidence
Whenever theoretical expectations and empirical observations disagree, either could
be at fault. On the empirical side, two potential sources of error can be readily
identi ed. The results of previous authors could be a uke, due to unusual characteristics of the data sets used (especially since, in several cases, the number of
data sets used was relatively small). Alternatively, these data sets might contain
no signi cant attribute dependences, and in this case the Bayesian classi er would
indeed be expected to perform well. In order to test both these hypotheses, we conducted an empirical study on 28 data sets, comparing the Bayesian classi er with
other learners, and measuring the degree of attribute dependence in the data sets.
The learners used were state-of-the art representatives of three major approaches
to classi cation learning: decision tree induction (C4.5 release 8, Quinlan, 1993),
instance-based learning (PEBLS 2.1, Cost & Salzberg, 1993) and rule induction
(CN2 version 6.1, Clark & Boswell, 1991). A simple Bayesian classi er was implemented for these experiments. Three main issues arise here: how to handle numeric
attributes, zero counts, and missing values. We deal with each in turn.
Numeric attributes were discretized into ten equal-length intervals (or one per

observed value, whichever was least). Although Dougherty et al. (1995) found
this approach to be slightly less accurate than a more informed one, it has the
advantage of simplicity, and is su cient for verifying that the Bayesian classi er
performs as well as, or better than, other learners. A version incorporating the
conventional assumption of Gaussian distributions was also implemented, for
purposes of comparison with the discretized one.
Zero counts are obtained when a given class and attribute value never occur

together in the training set, and can be problematic because the resulting zero
probabilities will wipe out the information in all the other probabilities P(Aj =
vjk jCi) when they are multiplied according to Equation 2. A principled solution
to this problem is to incorporate a small-sample correction into all probabilities,
such as the Laplace correction (Niblett, 1987). If nijk is the number of times

6

P. DOMINGOS AND M. PAZZANI

class Ci and value vjk of attribute Aj occur together, and ni is the total number
of times class Ci occurs in the training set, the uncorrected estimate of P(Aj =
vjk jCi) is nijk=ni , and the Laplace-corrected estimate is P(Aj = vjk jCi) =
(nijk + f)=(ni + fnj ), where nj is the number of values of attribute Aj (e.g.,
2 for a Boolean attribute), and f is a multiplicative factor. Following Kohavi,
Becker, and Sommer eld (1997), the Laplace correction was used with f = 1=n,
where n is the number of examples in the training set.
Missing values were ignored, both when computing counts for the probability
estimates and when classifying a test example. This ensures the Bayesian classi er does not inadvertently have access to more information than the other
algorithms, and if anything biases the results against it.

Twenty-eight data sets from the UCI repository (Merz, Murphy & Aha,1997)
were used in the study. Twenty runs were conducted for each data set, randomly
selecting 2 of the data for training and the remainder for testing. Table 1 shows
3
the average accuracies obtained. As a baseline, the default accuracies obtained by
guessing the most frequent class are also shown. Con dence levels for the observed
di erences in accuracy between the (discretized) Bayesian classi er and the other
algorithms, according to a one-tailed paired t test, are also reported.3
The results are summarized in Table 2. The rst line shows the number of domains
in which the Bayesian classi er was more accurate than the corresponding classi er,
versus the number in which it was less. For example, the Bayesian classi er was
more accurate than C4.5 in 19 domains, and less in 9. The second line considers only
those domains where the accuracy di erence was signi cant at the 5% level, using
a one-tailed paired t test. For example, the Bayesian classi er was signi cantly
more accurate than C4.5 in 12 data sets. According to both these measures, the
Bayesian classi er wins out over each of the other approaches. The third line shows
the con dence levels obtained by applying a binomial sign test to the results in the
rst line, and results in high con dence that the Bayesian classi er is more accurate
than C4.5 and CN2, if this sample of data sets is assumed to be representative.
The fourth line shows the con dence levels obtained by applying the more sensitive
Wilcoxon test (DeGroot, 1986) to the 28 average accuracy di erences obtained, and
results in high con dence that the Bayesian classi er is more accurate than each of
the other learners. The fth line shows the average accuracy across all data sets,
and again the Bayesian classi er performs the best. The last line shows the average
rank of each algorithm, computed for each domain by assigning rank 1 to the most
accurate algorithm, rank 2 to the second best, and so on. The Bayesian classi er is
the best-ranked of all algorithms, indicating that when it does not win it still tends
to be one of the best.
The comparative results of the discretized and Gaussian versions also con rm
the advantage of discretization, although on this larger ensemble of data sets the
di erence is less pronounced than that found by Dougherty et al. (1995), and the
Gaussian version also does quite well compared to the non-Bayesian learners.

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

7

Table 1. Classi cation accuracies and sample standard deviations, averaged over 20 random training/test splits. \Bayes" is the Bayesian classi er with discretization and \Gauss" is the Bayesian
classi er with Gaussian distributions. Superscripts denote con dence levels for the di erence in
accuracy between the Bayesian classi er and the correspondingalgorithm, using a one-tailed paired
t test: 1 is 99.5%, 2 is 99%, 3 is 97.5%, 4 is 95%, 5 is 90%, and 6 is below 90%.

Data Set

Bayes

Gauss

Audiology
73.0 6.1
73.0 6.1
Annealing
95.3 1.2
84.3 3.8
Breast cancer
71.6 4.7
71.3 4.3
Credit
84.5 1.8
78.9 2.5
Chess endgames 88.0 1.4
88.0 1.4
Diabetes
74.5 2.4
75.2 2.1
Echocardiogram 69.1 5.4
73.4 4.9
Glass
61.9 6.2
50.6 8.2
Heart disease
81.9 3.4
84.1 2.8
Hepatitis
85.3 3.7
85.2 4.0
Horse colic
80.7 3.7
79.3 3.7
Hypothyroid
97.5 0.3
97.9 0.4
93.2 3.5
93.9 1.9
Iris
Labor
91.3 4.9
88.7 10.66
Lung cancer
46.8 13.3 46.8 13.36
63.0 3.3
54.8 5.51
Liver disease
LED
62.9 6.5
62.9 6.56
Lymphography
81.6 5.9
81.1 4.86
Post-operative
64.7 6.8
67.2 5.03
Promoters
87.9 7.0
87.9 7.06
Primary tumor
44.2 5.5
44.2 5.56
Solar are
68.5 3.0
68.2 3.76
69.4 7.6
63.0 8.31
Sonar
Soybean
100.0 0.0 100.0 0.06
Splice junctions 95.4 0.6
95.4 0.66
91.2 1.7
91.2 1.76
Voting records
Wine
96.4 2.2
97.8 1.23
Zoology
94.4 4.1
94.1 3.86
6
1
6
1
6
6
1
1
1
6
1
1
6

C4.5

72.5
90.5
70.1
85.9
99.2
73.5
64.7
63.9
77.5
79.2
85.1
99.1
92.6
78.1
40.9
65.9
61.2
75.0
70.0
74.3
35.9
70.6
69.1
95.0
93.4
96.3
92.4
89.6

PEBLS

5.8
2.2
6.8
2.1
0.1
3.4
6.3
8.7
4.3
4.3
3.8
0.2
2.7
7.9
16.35
4.41
8.46
4.21
5.21
7.81
5.81
2.91
7.46
9.03
0.81
1.31
5.61
4.71

75.8
98.8
65.6
82.2
96.9
71.1
61.7
62.0
78.9
79.0
75.7
95.9
93.5
89.7
42.3
61.3
55.3
82.9
59.2
91.7
30.9
67.6
73.8
100.0
94.3
94.9
97.2
94.6

C4.5

PEBLS

6
1
5
3
1
5
1
6
1
1
1
1
6
1

5.4
0.8
4.7
1.9
0.7
2.4
6.4
7.4
4.0
5.1
5.0
0.7
3.0
5.0
17.36
4.36
6.11
5.66
8.02
5.93
4.71
3.56
7.41
0.06
0.51
1.21
1.86
4.36
3
1
1
1
1
1
1
6
1
1
1
1
6
6

CN2

71.0
81.2
67.9
82.0
98.1
73.8
68.2
63.8
79.7
80.3
82.5
98.8
93.3
82.1
38.6
65.0
58.6
78.8
60.8
75.9
39.8
70.4
66.2
96.9
81.5
95.8
90.8
90.6

Table 2. Summary of accuracy results.

Measure

No. wins
No. signif. wins
Sign test
Wilcoxon test
Average
Rank

Bayes Gauss
79.1
2.43

12-7
6-5
75.0
70.0
77.8
2.75

19-9
12-8
96.0
96.0
77.2
3.14

16-11
12-6
75.0
94.0
77.6
3.21

CN2
20-8
16-6
98.0
99.8
76.2
3.46

5.1
5.4
7.1
2.2
1.0
2.7
7.2
5.5
2.9
4.2
4.2
0.4
3.6
6.9
13.53
3.83
8.12
4.93
8.24
8.81
5.21
3.02
7.55
5.93
5.51
1.61
4.71
5.01
5
1
1
1
1
6
6
6
3
1
2
1
6
1

Def.
21.3
76.4
67.6
57.4
52.0
66.0
67.8
31.7
55.0
78.1
63.6
95.3
26.5
65.0
26.8
58.1
8.0
57.3
71.2
43.1
24.6
25.2
50.8
30.0
52.4
60.5
36.4
39.4

P. DOMINGOS AND M. PAZZANI

8

In summary, the present large-scale study con rms previous authors' observations on smaller ensembles of data sets; in fact, the current results are even more
favorable to the Bayesian classi er. However, this does not by itself disprove the
notion that the Bayesian classi er will only do well when attributes are independent given the class (or nearly so). As pointed out above, the Bayesian classi er's
good performance could simply be due to the absence of signi cant attribute dependences in the data. To investigate this, we need to measure the degree of attribute
dependence in the data in some way. Measuring high-order dependencies is di cult, because the relevant probabilities are apt to be very small, and not reliably
represented in the data. However, a rst and feasible approach consists in measuring pairwise dependencies (i.e., dependencies between pairs of attributes given the
class). Given attributes Am and An and the class variable C, a possible measure
of the degree of pairwise dependence between Am and An given C (Wan & Wong,
1989; Kononenko, 1991) is
D(Am ; AnjC) = H(Am jC) + H(AnjC) H(Am An jC);
(4)
where Am An represents the Cartesian product of attributes Am and An (i.e., a
derived attribute with one possible value corresponding to each combination of
values of Am and An ), and for all classes i and attribute values k,
X
X
H(Aj jC) = P(Ci)
P(Ci ^ Aj =vjk ) log2 P(Ci ^ Aj =vjk ):
(5)
i

k

The D(Am ; AnjC) measure is zero when Am and An are completely independent
given C, and increases with their degree of dependence, with the maximum occurring when the class and one attribute completely determine the other.4
D was computed for all classes and attribute pairs in each data set, using uniform discretization as before, ignoring missing values, and excluding pairings of an
attribute with itself. The results appear in Table 3.5 For comparison purposes, the
rst column shows the Bayesian classi er's rank in each domain (i.e., 1 if it was the
most accurate algorithm, 2 if it was the second most accurate, etc., ignoring the
Gaussian version). The second column shows the maximum value of D observed in
the data set. The third column shows the percentage of all attributes that exhibited a degree of dependence with some other attribute of at least 0.2. The fourth
column shows the average D for all attribute pairs in the data set.
This table leads to two important observations. One is that the Bayesian classi er
achieves higher accuracy than more sophisticated approaches in many domains
where there is substantial attribute dependence, and therefore the reason for its
good comparative performance is not that there are no attribute dependences in
the data. The other is that the correlation between the average degree of attribute
dependence and the di erence in accuracy between the Bayesian classi er and other
algorithms is very small (R2 = 0:04 for C4.5, 0.0004 for PEBLS, and 0.002 for
CN2), and therefore attribute dependence is not a good predictor of the Bayesian
classi er's di erential performance vs. approaches that can take it into account.
Given this empirical evidence, it is clear that a new theoretical understanding of
the Bayesian classi er is needed. We now turn to this.

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

9

Table 3. Empirical measures of attribute dependence.

Data Set

Breast cancer
Credit
Chess endgames
Diabetes
Echocardiogram
Glass
Heart disease
Hepatitis
Horse colic
Hypothyroid
Iris
Labor
Lung cancer
Liver disease
LED
Lymphography
Post-operative
Promoters
Solar are
Sonar
Soybean
Splice junctions
Voting records
Wine
Zoology

Rank Max. D % D> 0:2 Avg. D
1
2
4
1
1
4
1
1
3
3
3
1
1
3
1
2
2
2
3
2
1
1
4
2
2

0.548
0.790
0.383
0.483
0.853
0.836
0.388
0.899
2.780
2.777
0.731
1.514
1.226
0.513
0.060
0.410
0.181
0.394
0.216
1.471
0.726
0.084
0.316
0.733
0.150

66.7
46.7
25.0
62.5
85.7
100.0
53.8
57.9
100.0
60.0
100.0
100.0
98.2
100.0
0.0
55.6
0.0
98.2
16.7
100.0
31.4
0.0
25.0
100.0
0.0

0.093
0.063
0.015
0.146
0.450
0.363
0.085
0.103
0.286
0.095
0.469
0.474
0.165
0.243
0.025
0.076
0.065
0.149
0.041
0.491
0.016
0.017
0.052
0.459
0.021

4. An example of optimality without independence
Consider a Boolean concept, described by three attributes A, B and C. Assume
1
that the two classes, denoted by + and , are equiprobable (P(+) = P( ) = 2 ).
Given an example E, let P(Aj+) be a shorthand for P(A = aE j+), aE being the
value of attribute A in the instance, and similarly for the other attributes. Let A
and C be independent, and let A = B (i.e., A and B are completely dependent).
Therefore B should be ignored, and the optimal classi cation procedure for a test
instance is to assign it to class + if P(Aj+)P(C j+) P(Aj )P(C j ) > 0, to class
if the inequality has the opposite sign, and to an arbitrary class if the two sides
are equal. On the other hand, the Bayesian classi er will take B into account
as if it was independent from A, and this will be equivalent to counting A twice.
Thus, the Bayesian classi er will assign the instance to class + if P(Aj+)2 P(C j+)
P(Aj )2P(C j ) > 0, and to otherwise.
Applying Bayes' theorem, P(Aj+) can be reexpressed as P(A)P(+jA)=P (+), and
similarly for the other probabilities. Since P(+) = P( ), after canceling like terms
this leads to the equivalent expressions P(+jA)P(+jC) P( jA)P( jC) > 0 for
the optimal decision, and P(+jA)2 P(+jC) P( jA)2P( jC) > 0 for the Bayesian
classi er. Let P(+jA) = p and P(+jC) = q. Then class + should be selected

P. DOMINGOS AND M. PAZZANI

10

1
Simple Bayes
Optimal
0.8

q

0.6
0.4
0.2
0
0

0.2

0.4

0.6

0.8

1

p

Figure 1. Decision boundaries for the Bayesian classi er and the optimal classi er.

when pq (1 p) (1 q) > 0, which is equivalent to q > 1 p. With the Bayesian
classi er, it will be selected when p2q (1 p)2 (1 q) > 0, which is equivalent
2
to q > p2(1 p)p)2 . The two curves are shown in Figure 1. The remarkable fact
+(1
is that, even though the independence assumption is decisively violated because
B = A, the Bayesian classi er disagrees with the optimal procedure only in the two
narrow regions that are above one of the curves and below the other; everywhere
else it performs the correct classi cation. Thus, for all problems where (p; q) does
not fall in those two small regions, the Bayesian classi er is e ectively optimal. By
contrast, according to the independence assumption it should be optimal only when
the two expressions are identical, i.e. at the three isolated points where the curves
1
cross: (0, 1), ( 1 , 2 ) and (1, 0). This shows that the Bayesian classi er's range of
2
applicability may in fact be much broader than previously thought. In the next
section we examine the general case and formalize this result.

5. Local optimality
We begin with some necessary de nitions.
Definition 1 Let C(E) be the actual class of example E , and let CX (E) be the
class assigned to it by classi er X . Then the zero-one loss of X on E , denoted
LX (E), is de ned as

0 if CX (E)
LX (E) = 1 otherwise. = C(E)

(6)

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

11

Zero-one loss is an appropriate measure of performance when the task is classication, and it is the most frequently used one. It simply assigns a cost (loss) of
one to the failure to guess the correct class. In some situations, di erent types of
misclassi cation have di erent costs associated with them, and the use of a full
cost matrix, specifying a loss value for each (C(E); CX (E)) pair, will then be appropriate. (For example, in medical diagnosis the cost of diagnosing an ill patient
as healthy is generally di erent from that of diagnosing a healthy patient as ill.)
In practice, it often occurs that examples with exactly the same attribute values
have di erent classes. This re ects the fact that those attributes do not contain
all the information necessary to uniquely determine the class. In general, then,
an example E will not be associated with a single class, but rather with a vector
of class probabilities P(CijE), where the ith component represents the fraction of
times that E appears with class Ci. The zero-one loss or misclassi cation rate of
X on E is then more generally de ned as
LX (E) = 1 P(CX jE);

(7)

where CX (E), the class assigned by X to E, is abbreviated to CX for simplicity.
P(CX jE) is the accuracy of X on E. This de nition reduces to Equation 6 when
one class has probability 1 given E.
Definition 2 The Bayes rate for an example is the lowest zero-one loss achievable
by any classi er on that example (Duda & Hart, 1973).
Definition 3 A classi er is locally optimal for a given example i its zero-one
loss on that example is equal to the Bayes rate.
Definition 4 A classi er is globally optimal for a given sample (data set) i it is

locally optimal for every example in that sample. A classi er is globally optimal for
a given problem (domain) i it is globally optimal for all possible samples of that
problem (i.e., for all data sets extracted from that domain).

The use of zero-one loss for classi cation tasks should be contrasted with that of

squared error loss for probability estimation. This is de ned as

SEX (E) = P(C jE) PX (C jE)]2;

(8)

where X is the estimating procedure and C is the variable whose probability (or
probability density) we seek to estimate. If there is uncertainty associated with
P(C jE), the squared error loss is de ned as the expected value of the above expression. The main point of this article, shown in this section, can now be stated as
follows. When the independence assumption is violated, Equation 2 will in general
be suboptimal as a probability estimating procedure under the squared error loss
function, but combined with Equation 1 it can nevertheless still be optimal as a

P. DOMINGOS AND M. PAZZANI

12

classi cation procedure under the zero-one loss function. This result is a direct consequence of the di ering properties of these two loss measures: Equation 2 yields
minimal squared-error estimates of the class probabilities only when the estimates
are equal to the true values (i.e., when the independence assumption holds); but,
with Equation 1, it can yield minimal zero-one loss even when the class probability
estimates diverge widely from the true values, as long as the class with highest
estimated probability, CX (E), is the class with highest true probability.
For instance, suppose there are two classes + and , and let P(+jE) = 0:51
and P( jE) = 0:49 be the true class probabilities given example E. The optimal
classi cation decision is then to assign E to class + (i.e., to set CX (E) = +).
^
Suppose also that Equation 2 gives the estimates P(+jE) = f+ (E) = 0:99 and
^ jE) = f (E) = 0:01. The independence assumption is violated by a wide
P(
margin, and the squared-error loss is large, but the Bayesian classi er still makes
the optimal classi cation decision, minimizing the zero-one loss.
Consider the two-class Q in general. Let the classes be Q and as before,
case
+
p = P(+jE), r = P(+) a=1 P(Aj = vjk j+), and s = P( ) a=1 P(Aj = vjk j )
j
j
(refer to Equation 2). We will now derive a necessary and su cient condition for
the local optimality of the Bayesian classi er, and show that the volume of the
Bayesian classi er's region of optimality in the space of valid values of (p; r; s) is
half of this space's total volume.
Theorem 1 The Bayesian classi er is locally optimal under zero-one loss for an

example E i (p

1
2

^ r s) _ (p

1
2

^ r s) for E .

Proof: The Bayesian classi er is optimal when its zero-one loss is the minimum
possible. When p = P(+jE) > , the minimum loss is 1 p, and is obtained by
1
2

assigning E to class +. The Bayesian classi er assigns E to class + when f+ (E) >
1
f (E) according to Equation 2, i.e., when r > s. Thus if p > 2 ^ r > s the Bayesian
1
classi er is optimal. Conversely, when p = P(+jE) < 2 , the minimum zero-one loss
is p, and is obtained by assigning E to class , which the Bayesian classi er does
when r < s. Thus the Bayesian classi er is optimal when p < 1 ^ r < s. When
2
1
p = 2 , either decision is optimal, so the inequalities can be generalized as shown.

Note that this is not an asymptotic result: it is valid even when the probability
estimates used to compute r and s are obtained from nite samples.
Corollary 1 The Bayesian classi er is locally optimal under zero-one loss in half

the volume of the space of possible values of (p; r; s).

Proof: Since p is a probability, and r and s are products of probabilities, (p; r; s)
only takes values in the unit cube 0; 1]3. The region of this cube satisfying the
condition in Theorem 1 is shown shaded in Figure 2; it can easily be seen to occupy
half of the total volume of the cube. However, not all (r; s) pairs correspond to valid
probability combinations. Since p is unconstrained, the projection of the space U of

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

13

(1, 1, 1)

s

p

0

p > 1/2

r

s

r
s

0

p < 1/2

r

(0, 0, 0)

Figure 2. Region of optimality of the simple Bayesian classi er.

valid probability combinations on all planes p = p0 is the same. By Theorem 1, the
region of optimality on planes below p0 = 1 becomes the region of nonoptimality on
2
1
planes above p0 = 2 , and vice versa (i.e., the optimal region for projections below
p0 = 1 is the photographic negative of the optimal region for projections above).
2
Thus, if S is the area of U's projection and SO is the area of the optimal region for
1
p0 < 2 , the area of the optimal region for p0 > 1 is S SO , and the total volume of
2
the region of optimality is 1 SO + 1 (S SO ) = 1 S. (Also, since if (r; s) corresponds
2
2
2
to a valid probability combination then so does (s; r), the region of optimality is
1
1
symmetric about s = r, and therefore SO = 2 S both above and below p0 = 2 .)
In contrast, under squared error loss, Equation 2 is optimal as a set of probability
estimates P(CijE) only when the independence assumption holds, i.e., on the line
where the planes r = p and s = 1 p intersect. Thus the region of optimality of
Equation 2 under squared-error loss is a second-order in nitesimal fraction of its
region of optimality under zero-one loss. The Bayesian classi er is e ectively an
optimal predictor of the most likely class for a broad range of conditions in which the
independence assumption is violated. Previous notions of the Bayesian classi er's
limitations can now be seen as resulting from incorrectly applying intuitions based
on squared-error loss to the Bayesian classi er's performance under zero-one loss.

P. DOMINGOS AND M. PAZZANI

14

6. Global optimality
The extension of Theorem 1 to global optimality is immediate. Let p, r and s for
example E be indexed as pE , rE and sE .
Theorem 2 The Bayesian classi er is globally optimal under zero-one loss for a

sample (data set)

i

8 E 2 (pE

1
2

Proof: By De nition 4 and Theorem 1.

^ rE sE ) _ (pE

1
2

^ rE sE ).

However, verifying this condition directly on a test sample will in general not
be possible, since it involves nding the true class probabilities for all examples in
the sample. Further, verifying it for a given domain (i.e, for all possible samples
extracted from that domain) will in general involve a computation of size proportional to the number of possible examples, which is exponential in the number of
attributes, and therefore computationally infeasible. Thus the remainder of this
section is dedicated to investigating more concrete conditions for the global optimality of the Bayesian classi er, some necessary and some su cient. A zero-one
loss function is assumed throughout.

6.1. Necessary conditions
Let a be the number of attributes, as before, let c be the number of classes, let v be
the maximum number of values per attribute, and let d be the number of di erent
numbers representable on the machine implementing the Bayesian classi er. For
example, if numbers are represented using 16 bits, d = 216 = 65536.
Theorem 3 The Bayesian classi er cannot be globally optimal for more than

dc(av+1) di erent problems.
Proof: Since the Bayesian classi er's state is composed of c(av + 1) probabilities,
and each probability can only have d di erent values, the Bayesian classi er can
only be in at most dc(av+1) states, and thus it cannot distinguish between more
than this number of concepts.
Even though dc(av+1) can be very large, this is a signi cant restriction because
many concept classes have size doubly exponential in a (e.g., arbitrary DNF formulas in Boolean domains), and due to the extremely rapid growth of this function the
Bayesian classi er's capacity will be exceeded even for commonly-occurring values
of a. On the other hand, this restriction is compatible with concept classes whose
size grows only exponentially with a (e.g., conjunctions).
This result re ects the Bayesian classi er's limited capacity for information storage, and should be contrasted with the case of classi ers (like instance-based, rule
and decision tree learners) whose memory size can be proportional to the sample
size. It also shows that the condition in Theorem 2 is satis ed by an exponentially

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

15

decreasing fraction of all possible domains as a increases. This is consistent with
the fact that local optimality must be veri ed for every possible combination of
attribute values if the Bayesian classi er is to be globally optimal for a domain
(De nition 4), and the probability of this decreases exponentially with a, starting
at 100% for a = 1. However, a similar statement is true for other learners; it simply re ects the fact that it is very di cult to optimally learn a very wide class of
concepts. The information storage capacity of the Bayesian classi er is O(a). If
e is the training set size, learners that can memorize all the individual examples
(or the equivalent) have a storage capacity of O(ea), and therefore can in principle
converge to optimal when e ! 1. However, for any nite e there is a value of a
after which the fraction of problems on which those learners can be optimal also
starts to decrease exponentially with a.
Let a nominal attribute be de ned as one whose domain is nite and unordered, a
feature be de ned as an attribute with a given value (i.e., Aj =vjk is a feature), and a
set of classes be discriminable by a set of functions fi (E) if every possible example
E can be optimally classi ed by applying Equation 1 with this set of functions.
Then the following result is an immediate extension to the general nominal case of
a well-known one for Boolean attributes (Duda & Hart, 1973).
Theorem 4 When all attributes are nominal, the Bayesian classi er is not globally

optimal for classes that are not discriminable by linear functions of the corresponding features.

Proof: De ne one Boolean attribute bjk for each feature, i.e., bjk = 1 if Aj =
vjk and 0 otherwise, where vjk is the kth value of attribute Aj . Then, by taking the logarithm of Equation 2, the Bayesian classi er is equivalent to a linear machine P
(Duda & Hart, 1973) whose discriminant function for class Ci is
log P(Ci) + j;k log P(Aj = vjk jCi) bjk (i.e., the weight of each Boolean feature is the log-probability of the corresponding attribute value given the class).
This is not a su cient condition, because the Bayesian classi er cannot learn
some linearly separable concepts. For example, it fails for some m-of-n concepts,
even though they are linearly separable. An m-of-n concept is a Boolean concept
that is true if m or more out of the n attributes de ning the example space are
true. For example, if examples are described by three attributes A0 , A1 and A2 ,
the concept 2-of-3 is true if A0 and A1 are true, or A0 and A2 are true, or A1 and
A2 are true, or all three are true.6
Theorem 5 The Bayesian classi er is not globally optimal for m-of-n concepts.

Proof: This follows directly from the de nition of global optimality, and the fact

that there exist m-of-n concepts for which the Bayesian classi er makes errors, even
when the examples are noise-free (i.e., an example always has the same class) and
the Bayes rate is therefore zero (e.g., 3-of-7, Kohavi, 1995).

P. DOMINGOS AND M. PAZZANI

16

Let P(AjC) represent the probability that an arbitrary attribute A is true given
that the concept C is true, let a bar represent negation, and let all examples be
equally probable. In general, if the Bayesian classi er is trained with all 2n examples
of an m-of-n concept, and a test example has exactly j true-valued attributes, then
the Bayesian classi er will make a false positive error if Di (m; n; j) is positive and
j < m, and it will make a false negative error if Di (m; n; j) is negative and j m,
where
Di (m; n; j) = P(C) P(AjC)j 1 P(AjC)]n j

P(C) P(AjC)j 1 P(AjC)]n j
n
i
P(C) = i=m 2n
m 1
X n
i
P(C) = i=0 2n
n 1
X n 1
i
1
P(AjC) = i=mX
n
n
i=m i
n
X

m 2
X

n 1
i
P(AjC) = i=0 1
:
m
X n
i=0 i

For example, Di (8; 25; j) is positive for all j 6. Therefore, the Bayesian
classi er makes false positive errors for all examples that have 6 or 7 attributes
that are true. Similarly, Di (17; 25; j) is negative for all j 19 and the Bayesian
classi er makes false negative errors when there are 17 and 18 attributes that are
true. However, a simple modi cation of the Bayesian classi er will allow it to
perfectly discriminate all positive examples from negatives: adding a constant to
the discriminant function for the concept, or subtracting the same constant from
the discriminant function for its negation (Equation 1). We have implemented an
extension to the Bayesian classi er for two-class problems that nds the value of the
constant that maximizes predictive accuracy on the training data. In preliminary
experiments, we have observed that this extension achieves 100% accuracy on all
m-of-n concepts when trained on all 2n examples, for n less than 18. Furthermore,
we have tested this extension on the mushroom data set from the UCI repository
with 800 examples, and found that the average accuracy on 64 trials signi cantly

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

17

increased from 93.9% without this extension to 96.2% with this extension (with
99.9% con dence using a one-tailed paired t test).
Since in nominal domains the basic Bayesian classi er cannot learn some linearly
separable concepts, in these domains its range of optimality is a subset of the
perceptron's, or of a linear machine's (Duda & Hart, 1973). This leads to the
following result.
Let the Vapnik-Chervonenkis dimension, or VC dimension for short, be de ned
as in (Haussler, 1988).
Corollary 2 In domains composed of a nominal attributes, the VC dimension of

the simple Bayesian classi er is O(a).

Proof: This result follows immediately from Theorem 4 and the fact that, given

a attributes, the VC dimension of linear discriminant functions is O(a) (Haussler,
1988).

Thus, in nominal domains, the PAC-learning guarantees that apply to linear machines apply also to the Bayesian classi er. In particular, given a classi cation
problem for which the Bayesian classi er is optimal, the number of examples required for it to learn the required discrimination to within error with probability
1 is linear in the number of attributes a.
In numeric domains, the Bayesian classi er is not restricted to linearly separable
problems; for example, if classes are normally distributed, nonlinear boundaries
and multiple disconnected regions can arise, and the Bayesian classi er is able to
identify them (see Duda & Hart, 1973).

6.2. Su cient conditions
In this section we establish the Bayesian classi er's optimality for some common
concept classes.
Theorem 6 The Bayesian classi er is globally optimal if, for all classes Ci and
Q
examples E = (v1 ; v2; . . .; va), P(E jCi) = a=1 P(Aj = vj jCi).
j

This result was demonstrated in Section 1, and is restated here for completeness.
The crucial point is that this condition is su cient, but not necessary.
Theorem 7 The Bayesian classi er is globally optimal for learning conjunctions

of literals.

Proof: Suppose there are n literals Lj in the conjunction. A literal may be

a Boolean attribute or its negation. In addition, there may be a n irrelevant
attributes; they simply cause each row in the truth table to become 2a n rows
with the same values for the class and all relevant attributes, each of those rows
corresponding to a possible combination of the irrelevant attributes. For simplicity,

P. DOMINGOS AND M. PAZZANI

18

they will be ignored from here on (i.e., n = a will be assumed without loss of
generality). Recall that, in the truth table for conjunction, the class C is 0 (false)
for all but L0 = L1 = n = Ln = 1 (true). Thus, using a bar to denote negation,
2n
P(C) = 21n , P(C) = 2 2n 1 , P(Lj jC) = 1, P(Lj jC) = 0, P(Lj jC) = 2n 11 (the
number of times the literal is 0 in the truth table, divided by the number of times
the class is 0), and P(Lj jC) = 2nn 1 11 (the number of times the literal is 1 minus
2
the one time it corresponds to C, divided by the number of times the class is 0).
Let E be an arbitrary example, and let m of the conjunction's literals be true in
E. For simplicity, the factor 1=P (E) will be omitted from all probabilities. Then
we have
P(C jE) = P(C) P m (Lj jC) P n m(Lj jC) =
and

1
2n

0

P(C jE) = P(C) P m (Lj jC) P n m(Lj jC)
m 2n 1 n m
n 1
n
:
= 2 2n 1 22n 11
2n 1

if m = n
otherwise

n

1
Notice that 2nn 1 11 < 2 for all n. Thus, for m = n, P(C jE) = P(C) 2nn 1 11 <
2
2
1
P(C)( 2 )n < 21n = P(C jE), and class 1 wins. For all m < n, P(C jE) = 0 and
P(C jE) > 0, and thus class 0 wins. Therefore the Bayesian classi er always makes
the correct decision, i.e., it is globally optimal.

Conjunctive concepts satisfy the independence assumption for class 1, but not
for class 0. (For example, if C = A0 ^ A1 , P(A1jC) = 1 6= P(A1 jC; A0) = 0,
3
by inspection of the truth table.) Thus conjunctions are an example of a class of
concepts where the Bayesian classi er is in fact optimal, but would not be if it
required attribute independence.
This analysis assumes that the whole truth table is known, and that all examples
are equally likely. What will happen if either of these restrictions is removed?
Consider rst the case where examples are not distributed uniformly. For m < n,
the Bayesian classi er always produces the correct class, given a su cient sample.
For m = n, the result will, in general, depend on the distribution. The more
interesting and practical case occurs when P(C) > 21n , and in this case one can
easily verify that the Bayesian classi er continues to give the correct answers (and,
in fact, is now more robust with respect to sample uctuations). It will fail if
P(C) < 21n , but this is a very arti cial situation: in practice, examples of such a
conjunction would never appear in the data, or they would appear so infrequently
that learning the conjunction would be of little or no relevance to the accuracy.
At rst sight, the Bayesian classi er can also fail if the probabilities P(Lj jC)
are such that the product of all n such probabilities is greater than 21n (or, more

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

19

precisely, greater than P(C jE)=P (C)). P(Lj jC) can be increased by increasing the
frequency with which Lj is 1 but the class is not (i.e., at least one of the other literals
in the conjunction is 0). However, doing this necessarily decreases P(C), leading
to the arti cial situation just described. Further, because increasing P(Lj jC) also
decreases P(Lk jC) for the Lk that are 0 when Lj is 1 and the class is 1, it can be
shown that the product can never be greater than 21n . Thus, a very small P(C) is
e ectively the only situation where the Bayesian classi er will not be optimal. In
short, although distributional assumptions cannot be entirely removed, they can be
relaxed to exclude only the more pathological cases.
The Bayesian classi er's average-case behavior for insu cient samples (i.e., samples not including all possible examples) was analyzed by Langley et al. (1992),
who plotted sample cases and found the rate of convergence to 100% accuracy to be
quite rapid.7 Comparing Langley et al.'s results with Pazzani and Sarrett's (1990)
average-case formulas for the classical wholist algorithm for learning conjunctions
shows that the latter converges faster, which is not surprising, considering that it
was speci cally designed for this concept class. On the other hand, as Langley et al.
(1992) point out, the Bayesian classi er has the advantage of noise tolerance.
Theorem 8 The Bayesian classi er is globally optimal for learning disjunctions of

literals.

Proof: Similar to that for Theorem 7, letting m be the number of the disjunction's
literals that are false in E.

Conversely, disjunctions satisfy the independence assumption for class 0 but not
for class 1, and are another example of the Bayesian classi er's optimality even
when the independence assumption is violated.
As corollaries, the Bayesian classi er is also optimal for negated conjunctions and
negated disjunctions, as well as for the identity and negation functions, with any
number of irrelevant attributes.

7. When will the Bayesian classi er outperform other learners?
The previous sections showed that the Bayesian classi er is, in fact, optimal under
a far broader range of conditions than previously thought. However, even when
it is not optimal, the Bayesian classi er may still perform better than classi ers
with greater representational power, such as C4.5, PEBLS and CN2, with which it
was empirically compared in Section 3. Thus, a question of practical signi cance
arises: is it possible to identify conditions under which the Bayesian classi er can
be expected to do well, compared to these other classi ers? The current state
of knowledge in the eld does not permit a complete and rigorous answer to this
question, but some elements can be gleaned from the results in this article, and
from the literature.

20

P. DOMINGOS AND M. PAZZANI

It is well known that squared error loss can be decomposed into three additive
components (Friedman, 1996): the intrinsic error due to noise in the sample, the
statistical bias (systematic component of the approximation error, or error for an innite sample) and the variance (component of the error due to the approximation's
sensitivity to the sample, or error due to the sample's nite size). A trade-o exists
between bias and variance, and knowledge of it can often help in understanding
the relative behavior of estimation algorithms: those with greater representational
power, and thus greater ability to respond to the sample, tend to have lower bias,
but also higher variance.
Recently, several authors (Kong & Dietterich, 1995; Kohavi & Wolpert, 1996;
Tibshirani, 1996; Breiman, 1996; Friedman, 1996) have proposed similar biasvariance decompositions for zero-one loss functions. In particular, Friedman (1996)
has shown, using normal approximations to the class probabilities, that the biasvariance interaction now takes a very di erent form. Zero-one loss can be highly
insensitive to squared-error bias in the classi er's probability estimates, as Theorem 1 implies,8 but, crucially, will in general still be sensitive to estimation variance.
Thus, as long as Theorem 1's preconditions hold for most examples, a classi er with
high bias and low variance will tend to produce lower zero-one loss than one with
low bias and high variance, because only the variance's e ect will be felt. In this
way, the Bayesian classi er can often be a more accurate classi er than (say) C4.5,
even if in the in nite-sample limit the latter would provide a better approximation.
This may go a signi cant way towards explaining some of the results in Section 3.
This e ect should be especially visible at smaller sample sizes, since variance decreases with sample size. Indeed, Kohavi (1996) has observed that the Bayesian
classi er tends to outperform C4.5 on smaller data sets (hundreds to thousands
of examples), and conversely for larger ones (thousands to tens of thousands).
PAC-learning theory (e.g., Corollary 2) also lends support to this notion: even
though it provides only distribution-independent worst-case results, these suggest
that good performance on a small sample by the Bayesian classi er (or another
limited-capacity classi er) should be predictive of good out-of-sample accuracy,
while no similar statement can be made for classi ers with VC dimension on the
order of C4.5's. Further, since the VC dimension of a classi er typically increases
with the number of attributes, the Bayesian classi er should be particularly favored when, in addition to being small, the sample consists of examples described
by many attributes.
These hypotheses were tested by conducting experiments in arti cial domains.
The independent variables were the number of examples n and the number of attributes a, and the dependent variables were the accuracies of the Bayesian classi er
and C4.5. Concepts de ned as Boolean functions in disjunctive normal form (i.e.,
sets of rules) were used. The number of literals in each disjunct (i.e., the number
of conditions in each rule) was set according to a binomial distribution with mean
d and variance d(a d); this is obtained by including each attribute in the disjunct
with probability d=a (negated or not with equal probability). The number of disjuncts was set to 2d 1, so as to produce approximately equal numbers of positive

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

21

and negative examples, and positive examples were distributed evenly among the
disjuncts. The number of examples n was varied between 10 and 10000, and a was
varied between 16 and 64. A value of d = 8 was used, re ecting a bias for concepts
of intermediate complexity (d = 1 would produce the simplest concepts, and d = a
the most complex ones). One hundred di erent domains were generated at random for each (n; a) pair. For each domain, n examples were generated for training,
and 1000 for testing. Test-set accuracy was then averaged across domains. The
C4.5RULES postprocessor, which converts decision trees to rules and thus better
matches the target concept class, was used, and found to indeed increase accuracy,
by as much as 10% for larger n. All the results reported are for C4.5RULES.
The results appear graphically in Figure 3. All accuracy di erences are signi cant
with 99.9% con dence using a one-tailed paired t test.9 For this broad class of
domains, the Bayesian classi er is indeed more accurate than C4.5 at smaller sample
sizes (up to 1000, which includes many practical situations), and the crossover point
increases with the number of attributes, as does the Bayesian classi er's accuracy
advantage up to that point. These results are especially remarkable in light of
the fact that C4.5RULES's learning bias is far more appropriate to these domains
than the Bayesian classi er's, and illustrate how far variance can dominate bias
as a source of error in small to medium data sets. This can be seen as follows.
Since the Bayes rate is zero for these domains, the only components of the error
are bias and variance. If bias is taken to be the asymptotic error (i.e., the error
for an in nite sample), and variance the di erence between total error for a given
sample size and the bias (i.e., the \ nite sample penalty"), then C4.5's bias is zero,
since its accuracy asymptotes at 100%, and the Bayesian classi er has a high bias
(approximately 30{35%, depending on the number of attributes). On the other
hand, C4.5's variance, which approaches 50% for the smaller sample sizes, is much
higher than the Bayesian classi er's, and thus the sum of bias and variance for C4.5
is greater than that for the Bayesian classi er up to the crossover point.
Other authors have veri ed by Monte Carlo simulation that \choosing a simple
method of discrimination is often bene cial even if the underlying model assumptions are wrong" (Flury, Schmid, & Narayanan (1994) for quadratic discriminant
functions; Russek, Kronmal, & Fisher (1983) for the Bayesian classi er vs. multivariate Gaussian models). In general, the amount of structure that can be induced
for a domain will be limited by both the available sample and the learner's representational power. When the sample is the dominant limiting factor, a simple
learner like the Bayesian classi er may be better. However, as the sample size increases, the Bayesian classi er's capacity to store information about the domain
will be exhausted sooner than that of more powerful classi ers, and it may then
make sense to use the latter. Of course, the Bayesian classi er may still outperform other classi ers at larger samples sizes, if its learning bias happens to be more
appropriate for the domain.
The Bayesian classi er's exact degree of sensitivity to variance will depend on
the di erence r s, for r and s (see Section 5) estimated from an in nite sample.
If this di erence is large, errors in r and s due to small sample size will tend to

P. DOMINGOS AND M. PAZZANI

22

80
Bayes

Accuracy (%)

75

C4.5
70
65
60
55
50
10

100
1000
No. of examples

10000

100
1000
No. of examples

10000

100
1000
No. of examples

10000

80
Bayes

Accuracy (%)

75

C4.5
70
65
60
55
50
10
80
Bayes

Accuracy (%)

75

C4.5
70
65
60
55
50
10

Figure 3. Accuracy of the Bayesian classi er and C4.5RULES as a function of the number of
examples, given 16 attributes (upper), 32 attributes (middle), and 64 attributes (lower). Error
bars have a height of two standard deviations of the sample mean. All accuracy di erences are
signi cant with 99.9% con dence using a one-tailed paired t test.

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

23

leave the sign of r s unchanged, and thus have no e ect. On the other hand,
if r ' s, even small errors can cause the sign to change. If p and the in nitesample values of r and s satisfy the preconditions of Theorem 1, this will lead to
classi cation errors. Conversely, if they do not, this will lead to a reduction in
the misclassi cation rate, because incorrect classi cations will be ipped to correct
ones. Thus an increase in variance can sometimes lead to a reduction in zero-one
loss. Overall, Ben-Bassat, Klove, and Weil (1980) have shown that the Bayesian
classi er is quite robust with respect to errors in probability estimates due to small
sample size; this is not surprising, since it can be attributed to the same factors
that make it robust with respect to violations of the independence assumption.

8. How is the Bayesian classi er best extended?
One signi cant consequence of the Bayesian classi er's optimality even when strong
attribute dependences are present is that detecting these is not necessarily the best
way to improve performance. This section empirically tests this claim by comparing
Pazzani's (1996) extension with one that di ers from it solely by using the method
for attribute dependence detection described in (Kononenko, 1991) and (Wan &
Wong, 1989). In each case, the algorithm nds the single best pair of attributes
to join by considering all possible joins. Two measures for determining the best
pair were compared. Following Pazzani (1996), the rst measure was estimated
accuracy, as determined by leave-one-out cross validation on the training set. In
the second measure, Equation 4 was used to nd the attributes that had the largest
violation of the conditional independence assumption.
To conduct an experiment to compare these two approaches, a method is also required to decide when to stop joining attributes. Rather than selecting an arbitrary
threshold, experiments were conducted in two ways:
Joining only a single pair of attributes using each evaluation measure (provided
the change appeared bene cial to the measure).
With the cross-validation measure, joining of attributes stopped when no further
joining resulted in an improvement. With Equation 4, the optimal stopping
criterion was assumed to be given by an oracle. This was implemented by
selecting the threshold that performed best on the test data.
Two arti cial concepts were used to compare the approaches: exclusive OR with
two relevant attributes and six irrelevant attributes, and parity with six relevant
attributes and six irrelevant attributes. Experiments on UCI data sets were also
carried out, to determine whether the methods work on problems that occur in
practice as well as in arti cial concepts. In this set of experiments, a multiplicative factor of 1 was used for the Laplace correction (see Section 3), and numeric
attributes were discretized into ve equal intervals, instead of ten. This causes
the Cartesian product of two discretized attributes to have 25 values, instead of
100, and leads to substantially more reliable probability estimates, given that the

P. DOMINGOS AND M. PAZZANI

24

Table 4. A comparison of two approaches to extending the Bayesian classi er.

Data Set
Exclusive OR
4-parity
Chess endgames
Credit
Diabetes
Glass
Horse colic
Iris
Mushroom
Voting records
Wine
Wisconsin cancer

Training Bayes Accuracy Entropy Accuracy Entropy
Size
Once
Once Repeated Optimal
128
128
300
250
500
150
200
100
800
300
125
500

46.1
42.4
86.8
84.0
75.5
41.7
81.0
93.1
94.0
90.4
98.0
97.3

100.0
43.5
93.4 +
83.7
76.1
48.9 +
80.8
93.2
97.4 +
90.4
97.5
96.7

100.0
44.2
90.3
84.1
76.1
42.6
79.5
93.3
93.4
89.9
97.7
96.7

100.0
50.3
93.9 +
84.0
76.1
49.3 +
80.6
93.3
99.3 +
92.0
97.5
97.0

100.0
51.0
90.8
84.6
76.1
42.6
81.1
93.6
94.0
91.2
98.0
96.7

training set sizes are in the hundreds. The domains and training set sizes appear
in the rst two columns of Table 4. The remaining columns display the accuracy
of the Bayesian classi er and extensions, averaged over 24 paired trials, and found
by using an independent test set consisting of all examples not in the training set.
In Table 4, Accuracy Once shows results for the backward stepwise joining algorithm of Pazzani (1996), forming at most one Cartesian product as determined by
the highest accuracy using leave-one-out cross validation on the training set; Entropy Once is the same algorithm except it creates at most one Cartesian product
with the two attributes that have the highest degree of dependence. In this table,
a paired t test between these two algorithms is used to determine which method
has the highest accuracy when making a single change to the Bayesian classi er. A
\+" indicates that using one method is signi cantly more accurate than another.
Both algorithms do well on exclusive OR. In this case the joining of the two relevant attributes is clearly distinguished from others by either method. The results
indicate that estimating accuracy on the training data is signi cantly better on
three data sets and never signi cantly worse than using a measure of conditional
independence.
The column labeled Accuracy Repeated gives results for the backward sequential
joining algorithm; in contrast, Entropy Optimal repeats joining the pair of attributes
that have the highest degree of dependence, stopping when the dependences fall
below the optimal threshold to maximize accuracy on the test set. Paired t tests
indicate that the accuracy estimation approach is often signi cantly better than
using entropy to determine which attributes to join, and is never signi cantly worse.
To further explore whether the degree of dependence is a reasonable measure for
predicting which attributes to join, an additional experiment was performed on the
UCI data sets in which Cartesian product attributes were bene cial: we formed
every possible classi er with a single pair of joined attributes (and all remaining
attributes), and measured the test-set accuracy, the accuracy estimated by leave-

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

25

Test-set accuracy (%)

94
93
92
91
90
89
90

91
92
93
94
Cross-validation accuracy (%)

95

0.2
Degree of correlation

0.3

Test-set accuracy (%)

94
93
92
91
90
89
0.1

Figure 4. Upper: The relationship between accuracy on the test set and using accuracy estimation
on the training set to decide which Cartesian product attribute to form, plotted for all pairs of
attributes in the chess data set (R2 = 0.497). Lower: The relationship between accuracy on the
test set and using entropy to decide which Cartesian product attribute to form (R2 = 0.006).

one-out cross validation on the training set, and the degree of dependence. Figure 4
plots the accuracy of these classi ers on the test set as a function of the other
two measures (averaged over 24 trials) for the domain with the largest number of
attributes: chess endgames. The graphs show that cross-validation accuracy is a
better predictor of the e ect of an attribute join than the degree of dependence
given the class. The value of R2 for this domain was 0.497 for cross-validation
accuracy, vs. 0.006 for degree of dependence. For the voting domain, the values of
R2 were respectively 0.531 and 0.212, for the glass domain 0.242 and 0.001, and for
mushroom 0.907 and 0.019.

26

P. DOMINGOS AND M. PAZZANI

These experiments demonstrate that joining attributes to correct for the most
serious violations of the independence assumption does not necessarily yield the
most accurate classi er. To illustrate the reason for this nding, we constructed
examples of an arti cial concept with six variables. The concept is true whenever
two or more of A1 , A5, and A6 are true and two or more of A2, A3, and A4 are true.
We generated examples in which A1 had a 50% chance of being true, an all other
attributes Ai had a probability 1=i of having the same value as A1 . Otherwise, the
value was selected randomly with a 50% chance of being true. Therefore, attributes
A1 and A2 were the most dependent. To avoid problems of estimating probabilities
from small samples, we ran each algorithm on 500 examples generated as described
above and tested on a set of 500 examples generated in the same manner. We ran 24
trials of this procedure. Using this methodology, the simple Bayesian classi er was
only 92.8% accurate on this problem. When using the entropy-based approach to
nding a pair of attributes to join, A1 and A2 were always chosen, and the classi er
was signi cantly less accurate at 90.1%. In contrast, when using cross-validation
accuracy to determine which two attributes to join, A5 and A6 were always chosen.
These are the two least dependent attributes in the data, yet thhe accuracy of the
Bayesian classi er constructed in this manner was signi cantly higher, at 96.9%.
This occurs because on this problem the representational bias of the simple Bayesian
classi er presents more di culties than the independence assumption.
The experiments in this section show that the simple Bayesian classi er can be
productively extended. However, correcting the largest violation of the independence assumption does not necessarily result in the largest improvement. Rather,
since under zero-one loss the Bayesian classi er can tolerate some signi cant violations of the independence assumption, an approach that directly estimates the
e ect of the possible changes on this loss measure resulted in a more substantial
improvement.

9. Conclusions and future work
In this article we veri ed that the Bayesian classi er performs quite well in practice
even when strong attribute dependences are present. We also showed that this
follows at least partly from the fact that, contrary to previous assumptions, the
Bayesian classi er does not require attribute independence to be optimal under
zero-one loss. We then derived some necessary and some su cient conditions for
the Bayesian classi er's optimality. In particular, we showed that the Bayesian
classi er is an optimal learner for conjunctive and disjunctive concepts, even though
these violate the independence assumption. We hypothesized that the Bayesian
classi er may often be a better classi er than more powerful alternatives when the
sample size is small, even in domains where its learning model is not the most
appropriate one, and veri ed this by means of experiments in arti cial domains.
We also veri ed that searching for attribute dependences is not necessarily the best
approach to improving the Bayesian classi er's performance.

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

27

Ideally, we would like to have a complete set of necessary and su cient conditions
for the optimality of the Bayesian classi er, e ciently veri able on real problems.
In Section 6 we began the work towards this goal. Another important area of
future research concerns nding conditions under which the Bayesian classi er is
not optimal, but comes very close because it makes the wrong prediction on only a
small fraction of the examples. This should also shed further light on the discussion
in Section 7. Much work remains to be done in the continuation of this section,
further elucidating the conditions that will favor the Bayesian classi er over other
classi ers. Another useful extension of the present work would be to apply a similar
analysis to loss functions employing a full cost matrix (see Section 5).
In summary, the work reported here demonstrates that the Bayesian classi er
has much broader applicability than previously thought. Since it also has advantages in terms of simplicity, learning speed, classi cation speed, storage space and
incrementality, its use should perhaps be considered more often.

Acknowledgments
The rst author was partly supported by PRAXIS XXI and NATO scholarships.
The authors are grateful to the creators of the C4.5, PEBLS and CN2 systems, and
to all those who provided the data sets used in the empirical study. Please see the
documentation in the UCI Repository for detailed information.

Notes
1. If there is a tie, the class may be chosen randomly.
2. This article will not attempt to review work on the Bayesian classi er in the pattern recognition
literature. Journals where this work can be found include IEEE Transactions on Pattern
Analysis and Machine Intelligence, Pattern Recognition Letters, and Pattern Recognition.
3. These con dence levels should be interpreted with caution, due to the t test's assumption of
independentlydrawn samples. Thus, a 99% level for a data set means the Bayesian classi er can
be expected with high con dence to outperform the corresponding algorithm on training sets
drawn at random from that data set, since the accuracy results were obtained by independently
drawing training sets from the data set. This is useful for cross-checking the results of this study
with previous ones on the same data sets. However, no conclusions can be drawn regarding
di erent data sets drawn at random from the same domain as the UCI data set, because
with respect to the domain the training sets used here are not independent, being overlapping
subsets of the same data set. See Dietterich (1996) for more on this issue.
4. For any two attributes, Equations 4 and 5 implicitly marginalize over all other attributes. In
particular, they ignore that two dependent attributes could become independent given another
attribute or combination of attributes.
5. The annealing, audiology, and primary tumor domains are omitted because some of the relevant
entropies H (. . .) could not be computed. Due to a combination of missing values and rare
P ^
^
classes, for these data sets there exist Ci and Aj such that k P (Ci ^ Aj = vjk ) = 0 6= P (Ci ),
causing the entropy measure to become unde ned.

28

P. DOMINGOS AND M. PAZZANI

6. More generally, some attributes may be irrelevant, i.e., an m-of-n concept may be de ned using
only n < a attributes, where a is the total number of attributes describing the examples, and
one must then specify which attributes are the n relevant ones. This article considers only the
more restricted case, but the results can be trivially generalized.
7. The 100% asymptote implies optimality, but the authors did not remark on this fact.
8. Notice that Theorem 1 is valid for any classi er employing estimates r and s of the class
probabilities, not just the Bayesian classi er.
9. This includes points where the error bars overlap, which is possible because the t test is paired.
Also, note that these con dence levels apply to the accuracy di erence in the entire domain
class studied, not just a particular data set, since the training sets were drawn independently
from the domain class.

References
Ben-Bassat, M., Klove, K. L., & Weil, M. H. (1980). Sensitivity analysis in Bayesian classi cation models: Multiplicative deviations. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 2, 261{266.
Breiman, L. (1996). Bias, variance and arcing classi ers (Technical Report 460). Statistics
Department, University of California at Berkeley, Berkeley, CA. ftp://ftp.stat.berkeley.edu/users/breiman/arcall.ps.Z.
Cestnik, B. (1990). Estimating probabilities: A crucial task in machine learning. Proceedings of
the Ninth European Conference on Arti cial Intelligence. Stockholm, Sweden: Pitman.
Clark, P., & Boswell, R. (1991). Rule induction with CN2: Some recent improvements. Proceedings of the Sixth European Working Session on Learning (pp. 151{163). Porto, Portugal:
Springer-Verlag.
Clark, P., & Niblett, T. (1989). The CN2 induction algorithm. Machine Learning, 3, 261{283.
Cost, S., & Salzberg, S. (1993). A weighted nearest neighbor algorithm for learning with symbolic
features. Machine Learning, 10, 57{78.
DeGroot, M. H. (1986). Probability and statistics (2nd ed.). Reading, MA: Addison-Wesley.
Dietterich, T. (1996). Statistical tests for comparing supervised classi cation learning algorithms
(technical report). Department of Computer Science, Oregon State University, Corvallis, OR.
ftp://ftp.cs.orst.edu/pub/tgd/papers/stats.ps.gz.
Dougherty, J., Kohavi, R., & Sahami, M. (1995). Supervised and unsupervised discretization of
continuous features. Proceedings of the Twelfth International Conference on Machine Learning
(pp. 194{202). Tahoe City, CA: Morgan Kaufmann.
Duda, R. O., & Hart, P. E. (1973). Pattern classi cation and scene analysis. New York, NY:
Wiley.
Flury, B., Schmid, M. J., & Narayanan, A. (1994). Error rates in quadratic discrimination with
constraints on the covariance matrices. Journal of Classi cation, 11, 101{120.
Friedman, J. H. (1996). On bias, variance, 0/1 - loss, and the curse-of-dimensionality (technical
report). Department of Statistics, Stanford University, Stanford, CA. ftp://playfair.stanford.edu/pub/friedman/kdd.ps.Z.
Friedman, N., Geiger, D., & Goldszmidt, M. (1997). Bayesian network classi ers. Machine
Learning (this volume).
Haussler, D. (1988). Quantifying inductive bias: AI learning algorithms and Valiant's learning
framework. Arti cial Intelligence, 36, 177{221.
John, G., & Langley, P. (1995). Estimating continuous distributions in Bayesian classi ers. Proceedings of the Eleventh Conference on Uncertainty in Arti cial Intelligence (pp. 338{345).
Montreal, Canada: Morgan Kaufmann.
Kohavi, R. (1995). Wrappers for performance enhancement and oblivious decision graphs. PhD
thesis, Department of Computer Science, Stanford University, Stanford, CA.

OPTIMALITY OF THE SIMPLE BAYESIAN CLASSIFIER

29

Kohavi, R. (1996). Scaling up the accuracy of naive-Bayes classi ers: A decision-tree hybrid.
Proceedings of the Second International Conference on Knowledge Discovery and Data Mining
(pp. 202{207). Portland, OR: AAAI Press.
Kohavi, R., Becker, B., & Sommer eld, D. (1997). Improving simple Bayes (technical report).
Data Mining and Visualization Group, Silicon Graphics Inc., Mountain View, CA. ftp://starry.stanford.edu/pub/ronnyk/impSBC.ps.Z.
Kohavi, R., & Wolpert, D. H. (1996). Bias plus variance decomposition for zero-one loss functions.
Proceedings of the Thirteenth International Conference on Machine Learning (pp. 275{283).
Bari, Italy: Morgan Kaufmann.
Kong, E. B., & Dietterich, T. G. (1995). Error-correctingoutput coding corrects bias and variance.
Proceedings of the Twelfth International Conference on Machine Learning (pp. 313{321). Tahoe
City, CA: Morgan Kaufmann.
Kononenko, I. (1990). Comparison of inductive and naive Bayesian learning approaches to automatic knowledge acquisition. In B. Wielinga (Ed.), Current Trends in Knowledge Acquisition.
Amsterdam, The Netherlands: IOS Press.
Kononenko, I. (1991). Semi-naive Bayesian classi er. Proceedings of the Sixth European Working
Session on Learning (pp. 206{219). Porto, Portugal: Springer-Verlag.
Kubat, M., Flotzinger, D., & Pfurtscheller, G. (1993). Discovering patterns in EEG-Signals: Comparative study of a few methods. Proceedings of the Eighth European Conference on Machine
Learning (pp. 366{371). Vienna, Austria: Springer-Verlag.
Langley, P. (1993). Induction of recursive Bayesian classi ers. Proceedings of the Eighth European
Conference on Machine Learning (pp. 153{164). Vienna, Austria: Springer-Verlag.
Langley, P., Iba, W., & Thompson, K. (1992). An analysis of Bayesian classi ers. Proceedings of
the Tenth National Conference on Arti cial Intelligence (pp. 223{228). San Jose, CA: AAAI
Press.
Langley, P., & Sage, S. (1994). Induction of selective Bayesian classi ers. In Proceedings of the
Tenth Conference on Uncertainty in Arti cial Intelligence (pp. 399{406). Seattle, WA: Morgan
Kaufmann.
Merz, C. J., Murphy, P. M., & Aha, D. W. (1997). UCI repository of machine learning databases.
Department of Information and Computer Science, University of California, Irvine, CA. http://www.ics.uci.edu/ mlearn/MLRepository.html.
Niblett, T. (1987). Constructing decision trees in noisy domains. Proceedings of the Second
European Working Session on Learning (pp. 67{78). Bled, Yugoslavia: Sigma.
Pazzani, M. J. (1996). Searching for dependencies in Bayesian classi ers. In D. Fisher & H.-J.
Lenz (Eds.), Learning from data: Arti cial intelligence and statistics V (pp. 239{248). New
York, NY: Springer-Verlag.
Pazzani, M., Muramatsu, J., & Billsus, D. (1996). Syskill & Webert: Identifying interesting web
sites. Proceedings of the Thirteenth National Conference on Arti cial Intelligence (pp. 54{61).
Portland, OR: AAAI Press.
Pazzani, M., & Sarrett, W. (1990). A framework for average case analysis of conjunctive learning
algorithms. Machine Learning, 9, 349{372.
Quinlan, J. R. (1993). C4.5: Programs for machine learning. San Mateo, CA: Morgan Kaufmann.
Russek, E., Kronmal, R. A., & Fisher, L. D. (1983). The e ect of assuming independence in
applying Bayes' theorem to risk estimation and classi cation in diagnosis. Computers and
Biomedical Research, 16, 537{552.
Sahami, M. (1996). Learning limited dependence Bayesian classi ers. Proceedings of the Second
International Conference on Knowledge Discovery and Data Mining (pp. 335{338). Portland,
OR: AAAI Press.
Singh, M., & Provan, G. M. (1995). A comparison of induction algorithms for selective and nonselective Bayesian classi ers. Proceedings of the Twelfth International Conference on Machine
Learning (pp. 497{505). Tahoe City, CA: Morgan Kaufmann.
Singh, M., & Provan, G. M. (1996). E cient learning of selective Bayesian network classi ers.
Proceedings of the Thirteenth International Conference on Machine Learning (pp. 453{461).
Bari, Italy: Morgan Kaufmann.

30

P. DOMINGOS AND M. PAZZANI

Tibshirani, R. (1996). Bias, variance and prediction error for classi cation rules (technical report). Department of Preventive Medicine and Biostatistics, University of Toronto, Toronto,
Ontario. http://utstat.toronto.edu/reports/tibs/biasvar.ps.
Wan, S. J., & Wong, S. K. M. (1989). A measure for concept dissimilarity and its applications in
machine learning. Proceedings of the International Conference on Computing and Information
(pp. 267{273). Toronto, Ontario: North-Holland.
Received Date
Accepted Date
Final Manuscript Date

